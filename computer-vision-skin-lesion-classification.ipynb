{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing all the libraries\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport glob\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom torch.utils.data import WeightedRandomSampler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T08:02:46.894509Z","iopub.execute_input":"2025-12-22T08:02:46.895327Z","iopub.status.idle":"2025-12-22T08:02:46.899857Z","shell.execute_reply.started":"2025-12-22T08:02:46.895301Z","shell.execute_reply":"2025-12-22T08:02:46.898972Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"meta = pd.read_csv(\"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\")\nunique_lesion_ids = meta['lesion_id'].unique()\n\ntrain_ids, temp_ids = train_test_split(unique_lesion_ids, test_size=0.3)\ntest_ids, validation_ids = train_test_split(temp_ids, test_size=0.5)\n\ntrain_meta = meta[meta['lesion_id'].isin(train_ids)]\ntest_meta = meta[meta['lesion_id'].isin(test_ids)]\nvalidation_meta = meta[meta['lesion_id'].isin(validation_ids)]\n\nprint(f\"train ({100 * len(train_meta) / (len(train_meta) + len(test_meta) + len(validation_meta))}%):\\n{train_meta.head()}\\n\")\nprint(f\"test ({100 * len(test_meta) / (len(train_meta) + len(test_meta) + len(validation_meta))}%):\\n{test_meta.head()}\\n\")\nprint(f\"validation ({100 * len(validation_meta) / (len(train_meta) + len(test_meta) + len(validation_meta))}%):\\n{validation_meta.head()}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T08:02:46.901301Z","iopub.execute_input":"2025-12-22T08:02:46.901532Z","iopub.status.idle":"2025-12-22T08:02:46.943129Z","shell.execute_reply.started":"2025-12-22T08:02:46.901515Z","shell.execute_reply":"2025-12-22T08:02:46.942540Z"}},"outputs":[{"name":"stdout","text":"train (70.26460309535696%):\n     lesion_id      image_id   dx dx_type   age     sex localization\n2  HAM_0002730  ISIC_0026769  bkl   histo  80.0    male        scalp\n3  HAM_0002730  ISIC_0025661  bkl   histo  80.0    male        scalp\n4  HAM_0001466  ISIC_0031633  bkl   histo  75.0    male          ear\n5  HAM_0001466  ISIC_0027850  bkl   histo  75.0    male          ear\n8  HAM_0005132  ISIC_0025837  bkl   histo  70.0  female         back\n\ntest (14.927608587119321%):\n      lesion_id      image_id   dx dx_type   age     sex localization\n13  HAM_0001949  ISIC_0025767  bkl   histo  70.0    male        trunk\n14  HAM_0001949  ISIC_0032417  bkl   histo  70.0    male        trunk\n15  HAM_0007207  ISIC_0031326  bkl   histo  65.0    male         back\n20  HAM_0006071  ISIC_0032343  bkl   histo  70.0  female         face\n34  HAM_0005388  ISIC_0027815  bkl   histo  80.0    male        chest\n\nvalidation (14.807788317523714%):\n      lesion_id      image_id   dx dx_type   age   sex localization\n0   HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp\n1   HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp\n6   HAM_0002761  ISIC_0029176  bkl   histo  60.0  male         face\n7   HAM_0002761  ISIC_0029068  bkl   histo  60.0  male         face\n18  HAM_0007571  ISIC_0029836  bkl   histo  70.0  male        chest\n\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"class SkinCancerDataset(Dataset):\n    def __init__(self, dataframe, dir_part1, dir_part2, transform = None):\n        self.annotation = dataframe\n        self.dir_part1 = dir_part1\n        self.dir_part2 = dir_part2\n        self.transform = transform\n        self.label_map = {\n             'akiec': 0, \n             'bcc': 1, \n             'bkl': 2, \n             'df': 3, \n             'mel': 4, \n             'nv': 5, \n             'vasc': 6}\n\n    def __len__(self):\n        return len(self.annotation)\n\n    def __getitem__(self, index):\n        img_id = self.annotation.iloc[index]['image_id']#ImageID on column 2\n\n        path_part1 = os.path.join(self.dir_part1, img_id + '.jpg')\n        path_part2 = os.path.join(self.dir_part2, img_id + '.jpg')\n        if os.path.exists(path_part1):\n            img_name = path_part1\n        elif os.path.exists(path_part2):\n            img_name = path_part2\n        else:\n            raise FileNotFoundError(f\"Image {img_id} not found in part1 or part2\")\n            \n        image = Image.open(img_name).convert('RGB')\n        \n        label_text = self.annotation.iloc[index]['dx']\n        y_label = torch.tensor(self.label_map[label_text])\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y_label\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T08:02:46.944123Z","iopub.execute_input":"2025-12-22T08:02:46.944324Z","iopub.status.idle":"2025-12-22T08:02:46.950360Z","shell.execute_reply.started":"2025-12-22T08:02:46.944308Z","shell.execute_reply":"2025-12-22T08:02:46.949776Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"from torchvision import transforms\n\n# Training transforms (Randomness added)\ntrain_transforms = transforms.Compose([\n    transforms.Resize((96, 96)),       \n    transforms.RandomHorizontalFlip(),   \n    transforms.RandomVerticalFlip(),\n    transforms.RandomRotation(20),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n    transforms.ToTensor(),    \n    transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n    transforms.Normalize(                # Standardizes to ImageNet distribution\n        mean=[0.485, 0.456, 0.406], \n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n# Validation transforms (No Randomness, just resizing)\nval_transforms = transforms.Compose([\n    transforms.Resize((96, 96)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T08:02:46.951069Z","iopub.execute_input":"2025-12-22T08:02:46.951291Z","iopub.status.idle":"2025-12-22T08:02:46.967705Z","shell.execute_reply.started":"2025-12-22T08:02:46.951266Z","shell.execute_reply":"2025-12-22T08:02:46.967055Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"def get_device():\n    if torch.cuda.is_available():\n        print(f\"‚úÖ GPU Detected: {torch.cuda.get_device_name(0)}\")\n        return torch.device(\"cuda\")\n    \n    elif torch.backends.mps.is_available():\n        print(\"‚úÖ Apple Silicon GPU Detected\")\n        return torch.device(\"mps\")\n    \n    else:\n        print(\"‚ö†Ô∏è No GPU detected. Training will be slow.\")\n        return torch.device(\"cpu\")\n\n\ndevice = get_device()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T08:02:46.968840Z","iopub.execute_input":"2025-12-22T08:02:46.969025Z","iopub.status.idle":"2025-12-22T08:02:46.984984Z","shell.execute_reply.started":"2025-12-22T08:02:46.969012Z","shell.execute_reply":"2025-12-22T08:02:46.984327Z"}},"outputs":[{"name":"stdout","text":"‚úÖ GPU Detected: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"class DoubleConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n\n    def forward(self, x):\n        x = F.leaky_relu(self.bn1(self.conv1(x)))\n        x = F.leaky_relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)\n        return x\n        \n\nclass SkinCancerCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        #feature extraction\n        self.block1 = DoubleConvBlock(3, 32)\n        self.block2 = DoubleConvBlock(32, 64)\n        self.block3 = DoubleConvBlock(64, 128)\n        self.block4 = DoubleConvBlock(128, 256)\n        \n        self.flatten_size = 256 * 6 * 6\n        \n        # First fully connected layer\n        self.fc1 = nn.Linear(self.flatten_size, 512)\n        self.dropout = nn.Dropout(0.2)\n        # Second fully connected layer \n        self.fc2 = nn.Linear(512, 128)\n        \n        #third fully connected layer that outputs our 10 labels\n        self.fc3 = nn.Linear(128, 7)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        \n        #flatten the data\n        x = x.view(-1, self.flatten_size)\n        #FC layers\n        x = F.leaky_relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n\n        return x\n\nskinCancerCNN = SkinCancerCNN()\nprint(skinCancerCNN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T08:02:46.985794Z","iopub.execute_input":"2025-12-22T08:02:46.986025Z","iopub.status.idle":"2025-12-22T08:02:47.046551Z","shell.execute_reply.started":"2025-12-22T08:02:46.986010Z","shell.execute_reply":"2025-12-22T08:02:47.046017Z"}},"outputs":[{"name":"stdout","text":"SkinCancerCNN(\n  (block1): DoubleConvBlock(\n    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block2): DoubleConvBlock(\n    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block3): DoubleConvBlock(\n    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block4): DoubleConvBlock(\n    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc1): Linear(in_features=9216, out_features=512, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n  (fc2): Linear(in_features=512, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=7, bias=True)\n)\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(weight=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T08:02:47.047906Z","iopub.execute_input":"2025-12-22T08:02:47.048147Z","iopub.status.idle":"2025-12-22T08:02:47.051845Z","shell.execute_reply.started":"2025-12-22T08:02:47.048132Z","shell.execute_reply":"2025-12-22T08:02:47.051280Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"learning_rate = 0.0001\n\noptimizer = optim.Adam(skinCancerCNN.parameters(), learning_rate)\n\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T08:02:47.052374Z","iopub.execute_input":"2025-12-22T08:02:47.052542Z","iopub.status.idle":"2025-12-22T08:02:47.065454Z","shell.execute_reply.started":"2025-12-22T08:02:47.052528Z","shell.execute_reply":"2025-12-22T08:02:47.064816Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"# Define your paths (Adjust these to match your specific Kaggle input structure)\ndir_1 = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1/\"\ndir_2 = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2/\"\n\n# Create Datasets passing BOTH directories\ntrain_dataset = SkinCancerDataset(train_meta, dir_1, dir_2, transform=train_transforms)\nval_dataset = SkinCancerDataset(validation_meta, dir_1, dir_2, transform=val_transforms)\ntest_dataset = SkinCancerDataset(test_meta, dir_1, dir_2, transform=val_transforms)\n\n\n#create sampler weights\nlabels, counts = np.unique(meta['dx'], return_counts=True)\nweights = 1.0 / counts \nweights = weights / weights.sum() * len(counts)\nweights = torch.tensor(weights, dtype=torch.float)\n\nlabel_map = {\n    'akiec': 0,\n    'bcc' : 1,\n    'bkl' : 2,\n    'df' : 3,\n    'mel' : 4,\n    'nv' : 5,\n    'vasc' : 6,\n}\n\nprint(labels, label_map)\n\nsample_weights = [weights[label_map[label]] for label in train_meta['dx']]\n\nsampler = WeightedRandomSampler(weights=sample_weights,\n                                num_samples=len(sample_weights),\n                                replacement=True)\n\n# Create DataLoaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n                                           batch_size=32,\n                                           sampler=sampler)\n\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T08:02:47.066237Z","iopub.execute_input":"2025-12-22T08:02:47.066477Z","iopub.status.idle":"2025-12-22T08:02:47.270603Z","shell.execute_reply.started":"2025-12-22T08:02:47.066461Z","shell.execute_reply":"2025-12-22T08:02:47.270029Z"}},"outputs":[{"name":"stdout","text":"['akiec' 'bcc' 'bkl' 'df' 'mel' 'nv' 'vasc'] {'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3, 'mel': 4, 'nv': 5, 'vasc': 6}\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"import time\nfrom tqdm import tqdm # Library for progress bars\nimport matplotlib.pyplot as plt\n\n# ==========================================\n# CONFIGURATION\n# ==========================================\npatience = 10        # How many epochs to wait before stopping if no improvement\nmin_delta = 0.0005   # Minimum change to qualify as an improvement\nearly_stop_counter = 0\nbest_val_loss = float('inf')\nnum_epochs = 60\n\n# Move model to device\nmodel = skinCancerCNN.to(device)\n\n# History storage\nhistory = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n\nprint(f\"üöÄ Starting training on {device} with Early Stopping...\")\n\n# ==========================================\n# TRAINING LOOP\n# ==========================================\nfor epoch in range(num_epochs):\n    \n    # --- 1. Training Phase ---\n    model.train()\n    running_loss = 0.0\n    \n    # Wrap train_loader with tqdm for a progress bar\n    # 'desc' sets the text before the bar\n    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n    \n    for images, labels in loop:\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n        \n        # Update progress bar with current loss\n        loop.set_postfix(loss=loss.item())\n\n    epoch_loss = running_loss / len(train_loader.dataset)\n    history['train_loss'].append(epoch_loss)\n\n    # --- 2. Validation Phase ---\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * images.size(0)\n            \n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n    epoch_val_loss = val_loss / len(val_loader.dataset)\n    epoch_acc = 100 * correct / total\n    \n    history['val_loss'].append(epoch_val_loss)\n    history['val_acc'].append(epoch_acc)\n\n    # Print Clean Stats\n    print(f\"Epoch [{epoch+1}/{num_epochs}]  \"\n          f\"Train Loss: {epoch_loss:.4f} | \"\n          f\"Val Loss: {epoch_val_loss:.4f} | \"\n          f\"Val Acc: {epoch_acc:.2f}%\")\n\n    # ==========================================\n    # EARLY STOPPING LOGIC\n    # ==========================================\n    # Check if this validation loss is the best we've seen\n    if epoch_val_loss < (best_val_loss - min_delta):\n        best_val_loss = epoch_val_loss\n        early_stop_counter = 0 # Reset counter\n        torch.save(model.state_dict(), 'best_skin_cancer_model.pth')\n        print(f\"   ‚úÖ Validation Loss Improved. Model Saved.\")\n    else:\n        early_stop_counter += 1\n        print(f\"   ‚ö†Ô∏è No improvement for {early_stop_counter}/{patience} epochs.\")\n        \n    if early_stop_counter >= patience:\n        print(f\"\\nüõë Early Stopping Triggered! Training stopped at Epoch {epoch+1}.\")\n        break\n\nprint(\"Training Finished.\")\n\n# ==========================================\n# VISUALIZATION\n# ==========================================\ndef plot_training_history(history):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n    # Plot Loss\n    ax1.plot(history['train_loss'], label='Train Loss')\n    ax1.plot(history['val_loss'], label='Val Loss')\n    ax1.set_title('Loss History')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n    ax1.grid(True)\n\n    # Plot Accuracy\n    ax2.plot(history['val_acc'], label='Val Accuracy', color='green')\n    ax2.set_title('Validation Accuracy')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy (%)')\n    ax2.legend()\n    ax2.grid(True)\n\n    plt.show()\n\n# Run the plot\nplot_training_history(history)      \n       ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T08:02:47.271301Z","iopub.execute_input":"2025-12-22T08:02:47.271485Z"}},"outputs":[{"name":"stdout","text":"üöÄ Starting training on cuda with Early Stopping...\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [1/60]  Train Loss: 1.4863 | Val Loss: 1.0109 | Val Acc: 58.06%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [2/60]  Train Loss: 1.1733 | Val Loss: 1.0217 | Val Acc: 61.02%\n   ‚ö†Ô∏è No improvement for 1/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [3/60]  Train Loss: 1.1079 | Val Loss: 0.9840 | Val Acc: 59.20%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [4/60]  Train Loss: 1.0205 | Val Loss: 0.9251 | Val Acc: 60.49%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [5/60]  Train Loss: 0.9580 | Val Loss: 0.9247 | Val Acc: 65.21%\n   ‚ö†Ô∏è No improvement for 1/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [6/60]  Train Loss: 0.9191 | Val Loss: 0.9076 | Val Acc: 64.26%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [7/60]  Train Loss: 0.8610 | Val Loss: 0.8765 | Val Acc: 65.88%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [8/60]  Train Loss: 0.8198 | Val Loss: 0.8024 | Val Acc: 69.72%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [9/60]  Train Loss: 0.7919 | Val Loss: 0.8376 | Val Acc: 67.36%\n   ‚ö†Ô∏è No improvement for 1/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [10/60]  Train Loss: 0.7509 | Val Loss: 0.8743 | Val Acc: 66.22%\n   ‚ö†Ô∏è No improvement for 2/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [11/60]  Train Loss: 0.7378 | Val Loss: 0.8407 | Val Acc: 67.90%\n   ‚ö†Ô∏è No improvement for 3/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [12/60]  Train Loss: 0.7053 | Val Loss: 0.7978 | Val Acc: 70.26%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [13/60]  Train Loss: 0.6953 | Val Loss: 0.7601 | Val Acc: 71.27%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [14/60]  Train Loss: 0.6431 | Val Loss: 0.8767 | Val Acc: 66.62%\n   ‚ö†Ô∏è No improvement for 1/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [15/60]  Train Loss: 0.6633 | Val Loss: 0.8356 | Val Acc: 70.26%\n   ‚ö†Ô∏è No improvement for 2/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [16/60]  Train Loss: 0.6316 | Val Loss: 0.8475 | Val Acc: 67.70%\n   ‚ö†Ô∏è No improvement for 3/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [17/60]  Train Loss: 0.6051 | Val Loss: 0.8803 | Val Acc: 67.57%\n   ‚ö†Ô∏è No improvement for 4/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [18/60]  Train Loss: 0.5856 | Val Loss: 0.7344 | Val Acc: 71.61%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [19/60]  Train Loss: 0.5908 | Val Loss: 0.7636 | Val Acc: 70.94%\n   ‚ö†Ô∏è No improvement for 1/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [20/60]  Train Loss: 0.5758 | Val Loss: 0.7783 | Val Acc: 70.26%\n   ‚ö†Ô∏è No improvement for 2/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Epoch [21/60]:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 158/220 [00:48<00:19,  3.23it/s, loss=0.536]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # Confusion Matrix\nmodel = skinCancerCNN.to(device)\ny_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n\n# Load the weights from the file saved during training\n# (This requires 'best_skin_cancer_model.pth' to exist in your current folder)\ntry:\n    model.load_state_dict(torch.load('best_skin_cancer_model.pth'))\n    print(\"‚úÖ Successfully loaded the best saved model.\")\nexcept FileNotFoundError:\n    print(\"‚ö†Ô∏è File not found. Using current model weights instead.\")\n\nmodel.eval() # Set to evaluation mode (Turns off Dropout)\n\n# 2. PREDICTION LOOP: Get all predictions for the validation set\ny_true = []\ny_pred = []\n\nprint(\"üîÑ Processing validation set for Confusion Matrix...\")\n\nwith torch.no_grad(): # Save memory, we don't need gradients here\n    for images, labels in val_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        \n        # Get the class with the highest score\n        _, predicted = torch.max(outputs, 1)\n        \n        # Move back to CPU and convert to numpy for Sklearn\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(predicted.cpu().numpy())\n\nprint(\"‚úÖ Predictions complete.\")\n\n# 3. COMPUTE & PLOT: Create the Matrix\ncm = confusion_matrix(y_true, y_pred)\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=y_labels, yticklabels=y_labels)\nplt.xlabel('Predicted Label', fontsize=12)\nplt.ylabel('True Label', fontsize=12)\nplt.title('Confusion Matrix', fontsize=15)\nplt.show()\n\n# 4. REPORT: Print Precision, Recall, and F1-Score\nprint(\"\\nüìã Classification Report:\\n\")\nprint(classification_report(y_true, y_pred, target_names=y_labels))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}