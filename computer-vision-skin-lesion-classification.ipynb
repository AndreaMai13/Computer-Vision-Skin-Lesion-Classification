{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing all the libraries\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport glob\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom torch.utils.data import WeightedRandomSampler\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T13:21:40.253796Z","iopub.execute_input":"2025-12-22T13:21:40.254386Z","iopub.status.idle":"2025-12-22T13:21:40.259339Z","shell.execute_reply.started":"2025-12-22T13:21:40.254361Z","shell.execute_reply":"2025-12-22T13:21:40.258332Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"start_meta = pd.read_csv(\"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\")\n\n#start_meta = start_meta.drop(columns=['dx_type'])\nstart_meta['age'] = start_meta['age'].fillna(start_meta['age'].mean())\n\nscaler = StandardScaler()\nstart_meta['age'] = scaler.fit_transform(start_meta[['age']])\n\nencoder = OneHotEncoder(sparse_output=False) \ncat_cols = ['sex', 'localization']\nencoded_data_numpy = encoder.fit_transform(start_meta[cat_cols])\nfeature_names = encoder.get_feature_names_out(cat_cols)\nencoded_data = pd.DataFrame(encoded_data_numpy, columns=feature_names, index=start_meta.index)\n\nmeta = pd.concat([start_meta[['lesion_id', 'image_id', 'dx', 'age']], encoded_data], axis=1)\n\nunique_lesion_ids = meta['lesion_id'].unique()\n\ntrain_ids, temp_ids = train_test_split(unique_lesion_ids, test_size=0.3)\ntest_ids, validation_ids = train_test_split(temp_ids, test_size=0.5)\n\ntrain_meta = meta[meta['lesion_id'].isin(train_ids)]\ntest_meta = meta[meta['lesion_id'].isin(test_ids)]\nvalidation_meta = meta[meta['lesion_id'].isin(validation_ids)]\n\nprint(f\"train ({100 * len(train_meta) / (len(train_meta) + len(test_meta) + len(validation_meta))}%):\\n{train_meta.head()}\\n\")\nprint(f\"test ({100 * len(test_meta) / (len(train_meta) + len(test_meta) + len(validation_meta))}%):\\n{test_meta.head()}\\n\")\nprint(f\"validation ({100 * len(validation_meta) / (len(train_meta) + len(test_meta) + len(validation_meta))}%):\\n{validation_meta.head()}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T13:21:40.261281Z","iopub.execute_input":"2025-12-22T13:21:40.261720Z","iopub.status.idle":"2025-12-22T13:21:40.340337Z","shell.execute_reply.started":"2025-12-22T13:21:40.261704Z","shell.execute_reply":"2025-12-22T13:21:40.339596Z"}},"outputs":[{"name":"stdout","text":"train (70.22466300549176%):\n     lesion_id      image_id   dx       age  sex_female  sex_male  \\\n0  HAM_0000118  ISIC_0027419  bkl  1.662953         0.0       1.0   \n1  HAM_0000118  ISIC_0025030  bkl  1.662953         0.0       1.0   \n2  HAM_0002730  ISIC_0026769  bkl  1.662953         0.0       1.0   \n3  HAM_0002730  ISIC_0025661  bkl  1.662953         0.0       1.0   \n4  HAM_0001466  ISIC_0031633  bkl  1.367434         0.0       1.0   \n\n   sex_unknown  localization_abdomen  localization_acral  localization_back  \\\n0          0.0                   0.0                 0.0                0.0   \n1          0.0                   0.0                 0.0                0.0   \n2          0.0                   0.0                 0.0                0.0   \n3          0.0                   0.0                 0.0                0.0   \n4          0.0                   0.0                 0.0                0.0   \n\n   ...  localization_face  localization_foot  localization_genital  \\\n0  ...                0.0                0.0                   0.0   \n1  ...                0.0                0.0                   0.0   \n2  ...                0.0                0.0                   0.0   \n3  ...                0.0                0.0                   0.0   \n4  ...                0.0                0.0                   0.0   \n\n   localization_hand  localization_lower extremity  localization_neck  \\\n0                0.0                           0.0                0.0   \n1                0.0                           0.0                0.0   \n2                0.0                           0.0                0.0   \n3                0.0                           0.0                0.0   \n4                0.0                           0.0                0.0   \n\n   localization_scalp  localization_trunk  localization_unknown  \\\n0                 1.0                 0.0                   0.0   \n1                 1.0                 0.0                   0.0   \n2                 1.0                 0.0                   0.0   \n3                 1.0                 0.0                   0.0   \n4                 0.0                 0.0                   0.0   \n\n   localization_upper extremity  \n0                           0.0  \n1                           0.0  \n2                           0.0  \n3                           0.0  \n4                           0.0  \n\n[5 rows x 22 columns]\n\ntest (14.837743384922616%):\n      lesion_id      image_id   dx       age  sex_female  sex_male  \\\n33  HAM_0005612  ISIC_0024981  bkl  1.662953         0.0       1.0   \n37  HAM_0003847  ISIC_0030661  bkl  1.958471         0.0       1.0   \n38  HAM_0003847  ISIC_0027053  bkl  1.958471         0.0       1.0   \n39  HAM_0003847  ISIC_0028560  bkl  1.958471         0.0       1.0   \n40  HAM_0003847  ISIC_0031650  bkl  1.958471         0.0       1.0   \n\n    sex_unknown  localization_abdomen  localization_acral  localization_back  \\\n33          0.0                   0.0                 0.0                0.0   \n37          0.0                   0.0                 0.0                0.0   \n38          0.0                   0.0                 0.0                0.0   \n39          0.0                   0.0                 0.0                0.0   \n40          0.0                   0.0                 0.0                0.0   \n\n    ...  localization_face  localization_foot  localization_genital  \\\n33  ...                0.0                0.0                   0.0   \n37  ...                0.0                0.0                   0.0   \n38  ...                0.0                0.0                   0.0   \n39  ...                0.0                0.0                   0.0   \n40  ...                0.0                0.0                   0.0   \n\n    localization_hand  localization_lower extremity  localization_neck  \\\n33                0.0                           0.0                0.0   \n37                0.0                           0.0                0.0   \n38                0.0                           0.0                0.0   \n39                0.0                           0.0                0.0   \n40                0.0                           0.0                0.0   \n\n    localization_scalp  localization_trunk  localization_unknown  \\\n33                 1.0                 0.0                   0.0   \n37                 0.0                 0.0                   0.0   \n38                 0.0                 0.0                   0.0   \n39                 0.0                 0.0                   0.0   \n40                 0.0                 0.0                   0.0   \n\n    localization_upper extremity  \n33                           0.0  \n37                           1.0  \n38                           1.0  \n39                           1.0  \n40                           1.0  \n\n[5 rows x 22 columns]\n\nvalidation (14.93759360958562%):\n      lesion_id      image_id   dx       age  sex_female  sex_male  \\\n29  HAM_0001480  ISIC_0031753  bkl  1.071915         0.0       1.0   \n30  HAM_0001480  ISIC_0026835  bkl  1.071915         0.0       1.0   \n31  HAM_0005772  ISIC_0031159  bkl  0.480878         1.0       0.0   \n32  HAM_0005772  ISIC_0031017  bkl  0.480878         1.0       0.0   \n51  HAM_0007125  ISIC_0025016  bkl  1.367434         0.0       1.0   \n\n    sex_unknown  localization_abdomen  localization_acral  localization_back  \\\n29          0.0                   1.0                 0.0                0.0   \n30          0.0                   1.0                 0.0                0.0   \n31          0.0                   0.0                 0.0                0.0   \n32          0.0                   0.0                 0.0                0.0   \n51          0.0                   0.0                 0.0                1.0   \n\n    ...  localization_face  localization_foot  localization_genital  \\\n29  ...                0.0                0.0                   0.0   \n30  ...                0.0                0.0                   0.0   \n31  ...                1.0                0.0                   0.0   \n32  ...                1.0                0.0                   0.0   \n51  ...                0.0                0.0                   0.0   \n\n    localization_hand  localization_lower extremity  localization_neck  \\\n29                0.0                           0.0                0.0   \n30                0.0                           0.0                0.0   \n31                0.0                           0.0                0.0   \n32                0.0                           0.0                0.0   \n51                0.0                           0.0                0.0   \n\n    localization_scalp  localization_trunk  localization_unknown  \\\n29                 0.0                 0.0                   0.0   \n30                 0.0                 0.0                   0.0   \n31                 0.0                 0.0                   0.0   \n32                 0.0                 0.0                   0.0   \n51                 0.0                 0.0                   0.0   \n\n    localization_upper extremity  \n29                           0.0  \n30                           0.0  \n31                           0.0  \n32                           0.0  \n51                           0.0  \n\n[5 rows x 22 columns]\n\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"class SkinCancerDataset(Dataset):\n    def __init__(self, dataframe, dir_part1, dir_part2, transform = None):\n        self.annotation = dataframe\n        self.dir_part1 = dir_part1\n        self.dir_part2 = dir_part2\n        self.transform = transform\n        self.label_map = {\n             'akiec': 0, \n             'bcc': 1, \n             'bkl': 2, \n             'df': 3, \n             'mel': 4, \n             'nv': 5, \n             'vasc': 6}\n        self.meta_data_cols = self.annotation.drop(columns=[\n            'lesion_id',\n            'image_id',\n            'dx'])\n        \n    def __len__(self):\n        return len(self.annotation)\n\n    def __getitem__(self, index):\n        img_id = self.annotation.iloc[index]['image_id']#ImageID on column 2\n\n        path_part1 = os.path.join(self.dir_part1, img_id + '.jpg')\n        path_part2 = os.path.join(self.dir_part2, img_id + '.jpg')\n        if os.path.exists(path_part1):\n            img_name = path_part1\n        elif os.path.exists(path_part2):\n            img_name = path_part2\n        else:\n            raise FileNotFoundError(f\"Image {img_id} not found in part1 or part2\")\n            \n        image = Image.open(img_name).convert('RGB')\n        \n        label_text = self.annotation.iloc[index]['dx']\n        y_label = torch.tensor(self.label_map[label_text])\n\n        meta_data = self.meta_data_cols.iloc[index].values.astype('float32')\n        meta_data = torch.tensor(meta_data, dtype=torch.float32)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y_label, meta_data\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T13:21:40.341672Z","iopub.execute_input":"2025-12-22T13:21:40.341882Z","iopub.status.idle":"2025-12-22T13:21:40.348367Z","shell.execute_reply.started":"2025-12-22T13:21:40.341867Z","shell.execute_reply":"2025-12-22T13:21:40.347655Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"from torchvision import transforms\n\n# Training transforms (Randomness added)\ntrain_transforms = transforms.Compose([\n    transforms.Resize((96, 96)),       \n    transforms.RandomHorizontalFlip(),   \n    transforms.RandomVerticalFlip(),\n    transforms.RandomRotation(20),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n    transforms.ToTensor(),    \n    transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n    transforms.Normalize(                # Standardizes to ImageNet distribution\n        mean=[0.485, 0.456, 0.406], \n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n# Validation transforms (No Randomness, just resizing)\nval_transforms = transforms.Compose([\n    transforms.Resize((96, 96)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T13:21:40.349012Z","iopub.execute_input":"2025-12-22T13:21:40.349267Z","iopub.status.idle":"2025-12-22T13:21:40.366955Z","shell.execute_reply.started":"2025-12-22T13:21:40.349251Z","shell.execute_reply":"2025-12-22T13:21:40.366249Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"def get_device():\n    if torch.cuda.is_available():\n        print(f\"‚úÖ GPU Detected: {torch.cuda.get_device_name(0)}\")\n        return torch.device(\"cuda\")\n    \n    elif torch.backends.mps.is_available():\n        print(\"‚úÖ Apple Silicon GPU Detected\")\n        return torch.device(\"mps\")\n    \n    else:\n        print(\"‚ö†Ô∏è No GPU detected. Training will be slow.\")\n        return torch.device(\"cpu\")\n\n\ndevice = get_device()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T13:21:40.368088Z","iopub.execute_input":"2025-12-22T13:21:40.368288Z","iopub.status.idle":"2025-12-22T13:21:40.387019Z","shell.execute_reply.started":"2025-12-22T13:21:40.368267Z","shell.execute_reply":"2025-12-22T13:21:40.386433Z"}},"outputs":[{"name":"stdout","text":"‚úÖ GPU Detected: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"class DoubleConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n\n    def forward(self, x):\n        x = F.leaky_relu(self.bn1(self.conv1(x)))\n        x = F.leaky_relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)\n        return x\n        \n\nclass SkinCancerCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        #feature extraction\n        self.block1 = DoubleConvBlock(3, 32)\n        self.block2 = DoubleConvBlock(32, 64)\n        self.block3 = DoubleConvBlock(64, 128)\n        self.block4 = DoubleConvBlock(128, 256)\n        \n        self.flatten_size = 256 * 6 * 6 # --> + number of metadata?\n        \n        # First fully connected layer\n        self.fc1 = nn.Linear(self.flatten_size, 512)\n        self.dropout = nn.Dropout(0.3)\n        # Second fully connected layer \n        self.fc2 = nn.Linear(512 + 19, 128)\n        \n        #third fully connected layer that outputs our 10 labels\n        self.fc3 = nn.Linear(128, 7)\n\n    def forward(self, x, meta):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        \n        #flatten the data\n        x = x.view(-1, self.flatten_size)\n        #FC layers\n        x = F.leaky_relu(self.fc1(x))\n        x = self.dropout(x)\n\n        combined = torch.cat((x, meta), dim=1)\n        \n        x = F.leaky_relu(self.fc2(combined))\n        x = self.dropout(x)\n        x = self.fc3(x)\n\n        return x\n\nskinCancerCNN = SkinCancerCNN()\nprint(skinCancerCNN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T13:21:40.388486Z","iopub.execute_input":"2025-12-22T13:21:40.388825Z","iopub.status.idle":"2025-12-22T13:21:40.599880Z","shell.execute_reply.started":"2025-12-22T13:21:40.388797Z","shell.execute_reply":"2025-12-22T13:21:40.599092Z"}},"outputs":[{"name":"stdout","text":"SkinCancerCNN(\n  (block1): DoubleConvBlock(\n    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block2): DoubleConvBlock(\n    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block3): DoubleConvBlock(\n    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block4): DoubleConvBlock(\n    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc1): Linear(in_features=9216, out_features=512, bias=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc2): Linear(in_features=531, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=7, bias=True)\n)\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(weight=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T13:21:40.601331Z","iopub.execute_input":"2025-12-22T13:21:40.601745Z","iopub.status.idle":"2025-12-22T13:21:40.605177Z","shell.execute_reply.started":"2025-12-22T13:21:40.601728Z","shell.execute_reply":"2025-12-22T13:21:40.604546Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"learning_rate = 0.0001\n\noptimizer = optim.Adam(skinCancerCNN.parameters(), learning_rate)\n\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T13:21:40.605881Z","iopub.execute_input":"2025-12-22T13:21:40.606115Z","iopub.status.idle":"2025-12-22T13:21:40.618443Z","shell.execute_reply.started":"2025-12-22T13:21:40.606090Z","shell.execute_reply":"2025-12-22T13:21:40.617884Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# Define your paths (Adjust these to match your specific Kaggle input structure)\ndir_1 = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1/\"\ndir_2 = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2/\"\n\n# Create Datasets passing BOTH directories\ntrain_dataset = SkinCancerDataset(train_meta, dir_1, dir_2, transform=train_transforms)\nval_dataset = SkinCancerDataset(validation_meta, dir_1, dir_2, transform=val_transforms)\ntest_dataset = SkinCancerDataset(test_meta, dir_1, dir_2, transform=val_transforms)\n\nlabel_map = {\n    'akiec': 0,\n    'bcc' : 1,\n    'bkl' : 2,\n    'df' : 3,\n    'mel' : 4,\n    'nv' : 5,\n    'vasc' : 6,\n}\n\n#create sampler weights\nlabels, counts = np.unique(meta['dx'], return_counts=True)\nweights = 1.0 / np.sqrt(counts) \nweights = weights / weights.sum() * len(counts)\n\nprint(labels, label_map)\n\nsample_weights = [weights[label_map[label]] for label in train_meta['dx']]\n\nsampler = WeightedRandomSampler(weights=sample_weights,\n                                num_samples=len(sample_weights),\n                                replacement=True)\n\n# Create DataLoaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n                                           batch_size=32,\n                                           sampler=sampler)\n\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T13:21:40.619105Z","iopub.execute_input":"2025-12-22T13:21:40.619292Z","iopub.status.idle":"2025-12-22T13:21:40.645417Z","shell.execute_reply.started":"2025-12-22T13:21:40.619271Z","shell.execute_reply":"2025-12-22T13:21:40.644870Z"}},"outputs":[{"name":"stdout","text":"['akiec' 'bcc' 'bkl' 'df' 'mel' 'nv' 'vasc'] {'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3, 'mel': 4, 'nv': 5, 'vasc': 6}\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"import time\nfrom tqdm import tqdm # Library for progress bars\nimport matplotlib.pyplot as plt\n\n# ==========================================\n# CONFIGURATION\n# ==========================================\npatience = 10        # How many epochs to wait before stopping if no improvement\nmin_delta = 0.0005   # Minimum change to qualify as an improvement\nearly_stop_counter = 0\nbest_val_loss = float('inf')\nnum_epochs = 60\n\n# Move model to device\nmodel = skinCancerCNN.to(device)\n\n# History storage\nhistory = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n\nprint(f\"üöÄ Starting training on {device} with Early Stopping...\")\n\n# ==========================================\n# TRAINING LOOP\n# ==========================================\nfor epoch in range(num_epochs):\n    \n    # --- 1. Training Phase ---\n    model.train()\n    running_loss = 0.0\n    \n    # Wrap train_loader with tqdm for a progress bar\n    # 'desc' sets the text before the bar\n    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n    \n    for images, labels, meta_data in loop:\n        images, labels, meta_data = images.to(device), labels.to(device), meta_data.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images, meta_data)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n        \n        # Update progress bar with current loss\n        loop.set_postfix(loss=loss.item())\n\n    epoch_loss = running_loss / len(train_loader.dataset)\n    history['train_loss'].append(epoch_loss)\n\n    # --- 2. Validation Phase ---\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels, meta_data in val_loader:\n            images, labels, meta_data = images.to(device), labels.to(device), meta_data.to(device)\n            outputs = model(images, meta_data)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * images.size(0)\n            \n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n    epoch_val_loss = val_loss / len(val_loader.dataset)\n    epoch_acc = 100 * correct / total\n    \n    history['val_loss'].append(epoch_val_loss)\n    history['val_acc'].append(epoch_acc)\n\n    # Print Clean Stats\n    print(f\"Epoch [{epoch+1}/{num_epochs}]  \"\n          f\"Train Loss: {epoch_loss:.4f} | \"\n          f\"Val Loss: {epoch_val_loss:.4f} | \"\n          f\"Val Acc: {epoch_acc:.2f}%\")\n\n    # ==========================================\n    # EARLY STOPPING LOGIC\n    # ==========================================\n    # Check if this validation loss is the best we've seen\n    if epoch_val_loss < (best_val_loss - min_delta):\n        best_val_loss = epoch_val_loss\n        early_stop_counter = 0 # Reset counter\n        torch.save(model.state_dict(), 'best_skin_cancer_model.pth')\n        print(f\"   ‚úÖ Validation Loss Improved. Model Saved.\")\n    else:\n        early_stop_counter += 1\n        print(f\"   ‚ö†Ô∏è No improvement for {early_stop_counter}/{patience} epochs.\")\n        \n    if early_stop_counter >= patience:\n        print(f\"\\nüõë Early Stopping Triggered! Training stopped at Epoch {epoch+1}.\")\n        break\n\nprint(\"Training Finished.\")\n\n# ==========================================\n# VISUALIZATION\n# ==========================================\ndef plot_training_history(history):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n    # Plot Loss\n    ax1.plot(history['train_loss'], label='Train Loss')\n    ax1.plot(history['val_loss'], label='Val Loss')\n    ax1.set_title('Loss History')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n    ax1.grid(True)\n\n    # Plot Accuracy\n    ax2.plot(history['val_acc'], label='Val Accuracy', color='green')\n    ax2.set_title('Validation Accuracy')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy (%)')\n    ax2.legend()\n    ax2.grid(True)\n\n    plt.show()\n\n# Run the plot\nplot_training_history(history)      \n       ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T13:21:40.646672Z","iopub.execute_input":"2025-12-22T13:21:40.646856Z"}},"outputs":[{"name":"stdout","text":"üöÄ Starting training on cuda with Early Stopping...\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [1/60]  Train Loss: 1.4028 | Val Loss: 0.8955 | Val Acc: 63.37%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [2/60]  Train Loss: 1.1733 | Val Loss: 0.7916 | Val Acc: 69.99%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [3/60]  Train Loss: 1.0648 | Val Loss: 0.7892 | Val Acc: 69.92%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [4/60]  Train Loss: 1.0281 | Val Loss: 0.7041 | Val Acc: 74.73%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [5/60]  Train Loss: 1.0009 | Val Loss: 0.7563 | Val Acc: 70.59%\n   ‚ö†Ô∏è No improvement for 1/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [6/60]  Train Loss: 0.9653 | Val Loss: 0.7752 | Val Acc: 71.72%\n   ‚ö†Ô∏è No improvement for 2/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [7/60]  Train Loss: 0.9287 | Val Loss: 0.7379 | Val Acc: 73.13%\n   ‚ö†Ô∏è No improvement for 3/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [8/60]  Train Loss: 0.9040 | Val Loss: 0.6701 | Val Acc: 75.74%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [9/60]  Train Loss: 0.8757 | Val Loss: 0.6947 | Val Acc: 73.46%\n   ‚ö†Ô∏è No improvement for 1/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [10/60]  Train Loss: 0.8580 | Val Loss: 0.6842 | Val Acc: 74.20%\n   ‚ö†Ô∏è No improvement for 2/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [11/60]  Train Loss: 0.8494 | Val Loss: 0.6517 | Val Acc: 75.67%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [12/60]  Train Loss: 0.8050 | Val Loss: 0.6932 | Val Acc: 73.46%\n   ‚ö†Ô∏è No improvement for 1/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [13/60]  Train Loss: 0.7838 | Val Loss: 0.7013 | Val Acc: 73.93%\n   ‚ö†Ô∏è No improvement for 2/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [14/60]  Train Loss: 0.7665 | Val Loss: 0.6935 | Val Acc: 74.00%\n   ‚ö†Ô∏è No improvement for 3/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [15/60]  Train Loss: 0.7550 | Val Loss: 0.6980 | Val Acc: 74.13%\n   ‚ö†Ô∏è No improvement for 4/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [16/60]  Train Loss: 0.7391 | Val Loss: 0.6285 | Val Acc: 76.67%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [17/60]  Train Loss: 0.7313 | Val Loss: 0.6431 | Val Acc: 76.20%\n   ‚ö†Ô∏è No improvement for 1/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [18/60]  Train Loss: 0.7153 | Val Loss: 0.6649 | Val Acc: 75.67%\n   ‚ö†Ô∏è No improvement for 2/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"Epoch [19/60]:   2%|‚ñè         | 4/220 [00:01<01:04,  3.37it/s, loss=0.739]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":" # Confusion Matrix\nmodel = skinCancerCNN.to(device)\ny_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n\n# Load the weights from the file saved during training\n# (This requires 'best_skin_cancer_model.pth' to exist in your current folder)\ntry:\n    model.load_state_dict(torch.load('best_skin_cancer_model.pth'))\n    print(\"‚úÖ Successfully loaded the best saved model.\")\nexcept FileNotFoundError:\n    print(\"‚ö†Ô∏è File not found. Using current model weights instead.\")\n\nmodel.eval() # Set to evaluation mode (Turns off Dropout)\n\n# 2. PREDICTION LOOP: Get all predictions for the validation set\ny_true = []\ny_pred = []\n\nprint(\"üîÑ Processing validation set for Confusion Matrix...\")\n\nwith torch.no_grad(): # Save memory, we don't need gradients here\n    for images, labels in val_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        \n        # Get the class with the highest score\n        _, predicted = torch.max(outputs, 1)\n        \n        # Move back to CPU and convert to numpy for Sklearn\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(predicted.cpu().numpy())\n\nprint(\"‚úÖ Predictions complete.\")\n\n# 3. COMPUTE & PLOT: Create the Matrix\ncm = confusion_matrix(y_true, y_pred)\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=y_labels, yticklabels=y_labels)\nplt.xlabel('Predicted Label', fontsize=12)\nplt.ylabel('True Label', fontsize=12)\nplt.title('Confusion Matrix', fontsize=15)\nplt.show()\n\n# 4. REPORT: Print Precision, Recall, and F1-Score\nprint(\"\\nüìã Classification Report:\\n\")\nprint(classification_report(y_true, y_pred, target_names=y_labels))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}