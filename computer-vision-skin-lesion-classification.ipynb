{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing all the libraries\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport glob\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom torch.utils.data import WeightedRandomSampler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:38:05.429336Z","iopub.execute_input":"2025-12-22T09:38:05.429667Z","iopub.status.idle":"2025-12-22T09:38:05.434577Z","shell.execute_reply.started":"2025-12-22T09:38:05.429644Z","shell.execute_reply":"2025-12-22T09:38:05.433919Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"meta = pd.read_csv(\"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\")\nunique_lesion_ids = meta['lesion_id'].unique()\n\ntrain_ids, temp_ids = train_test_split(unique_lesion_ids, test_size=0.3)\ntest_ids, validation_ids = train_test_split(temp_ids, test_size=0.5)\n\ntrain_meta = meta[meta['lesion_id'].isin(train_ids)]\ntest_meta = meta[meta['lesion_id'].isin(test_ids)]\nvalidation_meta = meta[meta['lesion_id'].isin(validation_ids)]\n\nprint(f\"train ({100 * len(train_meta) / (len(train_meta) + len(test_meta) + len(validation_meta))}%):\\n{train_meta.head()}\\n\")\nprint(f\"test ({100 * len(test_meta) / (len(train_meta) + len(test_meta) + len(validation_meta))}%):\\n{test_meta.head()}\\n\")\nprint(f\"validation ({100 * len(validation_meta) / (len(train_meta) + len(test_meta) + len(validation_meta))}%):\\n{validation_meta.head()}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:38:05.435709Z","iopub.execute_input":"2025-12-22T09:38:05.436037Z","iopub.status.idle":"2025-12-22T09:38:05.479874Z","shell.execute_reply.started":"2025-12-22T09:38:05.436019Z","shell.execute_reply":"2025-12-22T09:38:05.479257Z"}},"outputs":[{"name":"stdout","text":"train (69.69545681477783%):\n     lesion_id      image_id   dx dx_type   age   sex localization\n0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp\n1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp\n2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp\n3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp\n4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear\n\ntest (15.197204193709435%):\n      lesion_id      image_id   dx dx_type   age     sex localization\n6   HAM_0002761  ISIC_0029176  bkl   histo  60.0    male         face\n7   HAM_0002761  ISIC_0029068  bkl   histo  60.0    male         face\n10  HAM_0001396  ISIC_0025276  bkl   histo  55.0  female        trunk\n13  HAM_0001949  ISIC_0025767  bkl   histo  70.0    male        trunk\n14  HAM_0001949  ISIC_0032417  bkl   histo  70.0    male        trunk\n\nvalidation (15.10733899151273%):\n      lesion_id      image_id   dx dx_type   age     sex     localization\n8   HAM_0005132  ISIC_0025837  bkl   histo  70.0  female             back\n9   HAM_0005132  ISIC_0025209  bkl   histo  70.0  female             back\n16  HAM_0001601  ISIC_0025915  bkl   histo  75.0    male  upper extremity\n17  HAM_0001601  ISIC_0031029  bkl   histo  75.0    male  upper extremity\n20  HAM_0006071  ISIC_0032343  bkl   histo  70.0  female             face\n\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"class SkinCancerDataset(Dataset):\n    def __init__(self, dataframe, dir_part1, dir_part2, transform = None):\n        self.annotation = dataframe\n        self.dir_part1 = dir_part1\n        self.dir_part2 = dir_part2\n        self.transform = transform\n        self.label_map = {\n             'akiec': 0, \n             'bcc': 1, \n             'bkl': 2, \n             'df': 3, \n             'mel': 4, \n             'nv': 5, \n             'vasc': 6}\n\n    def __len__(self):\n        return len(self.annotation)\n\n    def __getitem__(self, index):\n        img_id = self.annotation.iloc[index]['image_id']#ImageID on column 2\n\n        path_part1 = os.path.join(self.dir_part1, img_id + '.jpg')\n        path_part2 = os.path.join(self.dir_part2, img_id + '.jpg')\n        if os.path.exists(path_part1):\n            img_name = path_part1\n        elif os.path.exists(path_part2):\n            img_name = path_part2\n        else:\n            raise FileNotFoundError(f\"Image {img_id} not found in part1 or part2\")\n            \n        image = Image.open(img_name).convert('RGB')\n        \n        label_text = self.annotation.iloc[index]['dx']\n        y_label = torch.tensor(self.label_map[label_text])\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y_label\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:38:05.480958Z","iopub.execute_input":"2025-12-22T09:38:05.481200Z","iopub.status.idle":"2025-12-22T09:38:05.487497Z","shell.execute_reply.started":"2025-12-22T09:38:05.481184Z","shell.execute_reply":"2025-12-22T09:38:05.486872Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"from torchvision import transforms\n\n# Training transforms (Randomness added)\ntrain_transforms = transforms.Compose([\n    transforms.Resize((96, 96)),       \n    transforms.RandomHorizontalFlip(),   \n    transforms.RandomVerticalFlip(),\n    transforms.RandomRotation(20),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n    transforms.ToTensor(),    \n    transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n    transforms.Normalize(                # Standardizes to ImageNet distribution\n        mean=[0.485, 0.456, 0.406], \n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n# Validation transforms (No Randomness, just resizing)\nval_transforms = transforms.Compose([\n    transforms.Resize((96, 96)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:38:05.488160Z","iopub.execute_input":"2025-12-22T09:38:05.488691Z","iopub.status.idle":"2025-12-22T09:38:05.507044Z","shell.execute_reply.started":"2025-12-22T09:38:05.488668Z","shell.execute_reply":"2025-12-22T09:38:05.506129Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"def get_device():\n    if torch.cuda.is_available():\n        print(f\"‚úÖ GPU Detected: {torch.cuda.get_device_name(0)}\")\n        return torch.device(\"cuda\")\n    \n    elif torch.backends.mps.is_available():\n        print(\"‚úÖ Apple Silicon GPU Detected\")\n        return torch.device(\"mps\")\n    \n    else:\n        print(\"‚ö†Ô∏è No GPU detected. Training will be slow.\")\n        return torch.device(\"cpu\")\n\n\ndevice = get_device()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:38:05.508408Z","iopub.execute_input":"2025-12-22T09:38:05.508738Z","iopub.status.idle":"2025-12-22T09:38:05.523311Z","shell.execute_reply.started":"2025-12-22T09:38:05.508715Z","shell.execute_reply":"2025-12-22T09:38:05.522630Z"}},"outputs":[{"name":"stdout","text":"‚úÖ GPU Detected: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"class DoubleConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n\n    def forward(self, x):\n        x = F.leaky_relu(self.bn1(self.conv1(x)))\n        x = F.leaky_relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)\n        return x\n        \n\nclass SkinCancerCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        #feature extraction\n        self.block1 = DoubleConvBlock(3, 32)\n        self.block2 = DoubleConvBlock(32, 64)\n        self.block3 = DoubleConvBlock(64, 128)\n        self.block4 = DoubleConvBlock(128, 256)\n        \n        self.flatten_size = 256 * 6 * 6\n        \n        # First fully connected layer\n        self.fc1 = nn.Linear(self.flatten_size, 512)\n        self.dropout = nn.Dropout(0.2)\n        # Second fully connected layer \n        self.fc2 = nn.Linear(512, 128)\n        \n        #third fully connected layer that outputs our 10 labels\n        self.fc3 = nn.Linear(128, 7)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        \n        #flatten the data\n        x = x.view(-1, self.flatten_size)\n        #FC layers\n        x = F.leaky_relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.leaky_relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n\n        return x\n\nskinCancerCNN = SkinCancerCNN()\nprint(skinCancerCNN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:38:05.564277Z","iopub.execute_input":"2025-12-22T09:38:05.564697Z","iopub.status.idle":"2025-12-22T09:38:05.621328Z","shell.execute_reply.started":"2025-12-22T09:38:05.564679Z","shell.execute_reply":"2025-12-22T09:38:05.620527Z"}},"outputs":[{"name":"stdout","text":"SkinCancerCNN(\n  (block1): DoubleConvBlock(\n    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block2): DoubleConvBlock(\n    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block3): DoubleConvBlock(\n    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block4): DoubleConvBlock(\n    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc1): Linear(in_features=9216, out_features=512, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n  (fc2): Linear(in_features=512, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=7, bias=True)\n)\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"\ncriterion = nn.CrossEntropyLoss(weight=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:38:05.622472Z","iopub.execute_input":"2025-12-22T09:38:05.622692Z","iopub.status.idle":"2025-12-22T09:38:05.626983Z","shell.execute_reply.started":"2025-12-22T09:38:05.622675Z","shell.execute_reply":"2025-12-22T09:38:05.626325Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"learning_rate = 0.0005\n\noptimizer = optim.Adam(skinCancerCNN.parameters(), learning_rate)\n\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:38:05.627743Z","iopub.execute_input":"2025-12-22T09:38:05.627979Z","iopub.status.idle":"2025-12-22T09:38:05.641346Z","shell.execute_reply.started":"2025-12-22T09:38:05.627957Z","shell.execute_reply":"2025-12-22T09:38:05.640733Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# Define your paths (Adjust these to match your specific Kaggle input structure)\ndir_1 = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1/\"\ndir_2 = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2/\"\n\n# Create Datasets passing BOTH directories\ntrain_dataset = SkinCancerDataset(train_meta, dir_1, dir_2, transform=train_transforms)\nval_dataset = SkinCancerDataset(validation_meta, dir_1, dir_2, transform=val_transforms)\ntest_dataset = SkinCancerDataset(test_meta, dir_1, dir_2, transform=val_transforms)\n\nlabel_map = {\n    'akiec': 0,\n    'bcc' : 1,\n    'bkl' : 2,\n    'df' : 3,\n    'mel' : 4,\n    'nv' : 5,\n    'vasc' : 6,\n}\n\nprint(labels, label_map)\n\n#create sampler weights\nlabels, counts = np.unique(meta['dx'], return_counts=True)\nweights = 1.0 / np.sqrt(counts) \nweights = weights / weights.sum() * len(counts)\n\nsample_weights = [weights[label_map[label]] for label in train_meta['dx']]\n\nsampler = WeightedRandomSampler(weights=sample_weights,\n                                num_samples=len(sample_weights),\n                                replacement=True)\n\n# Create DataLoaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n                                           batch_size=32,\n                                           sampler=sampler)\n\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:38:05.642056Z","iopub.execute_input":"2025-12-22T09:38:05.642249Z","iopub.status.idle":"2025-12-22T09:38:05.664259Z","shell.execute_reply.started":"2025-12-22T09:38:05.642235Z","shell.execute_reply":"2025-12-22T09:38:05.663465Z"}},"outputs":[{"name":"stdout","text":"tensor([6, 2, 5, 2, 0, 2, 2, 0, 5, 0, 0, 4, 1, 5, 4, 5, 6, 2, 1, 2, 3, 5, 6, 4,\n        0, 3, 1, 6, 5, 3, 3, 3], device='cuda:0') {'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3, 'mel': 4, 'nv': 5, 'vasc': 6}\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"import time\nfrom tqdm import tqdm # Library for progress bars\nimport matplotlib.pyplot as plt\n\n# ==========================================\n# CONFIGURATION\n# ==========================================\npatience = 10        # How many epochs to wait before stopping if no improvement\nmin_delta = 0.0005   # Minimum change to qualify as an improvement\nearly_stop_counter = 0\nbest_val_loss = float('inf')\nnum_epochs = 60\n\n# Move model to device\nmodel = skinCancerCNN.to(device)\n\n# History storage\nhistory = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n\nprint(f\"üöÄ Starting training on {device} with Early Stopping...\")\n\n# ==========================================\n# TRAINING LOOP\n# ==========================================\nfor epoch in range(num_epochs):\n    \n    # --- 1. Training Phase ---\n    model.train()\n    running_loss = 0.0\n    \n    # Wrap train_loader with tqdm for a progress bar\n    # 'desc' sets the text before the bar\n    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n    \n    for images, labels in loop:\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n        \n        # Update progress bar with current loss\n        loop.set_postfix(loss=loss.item())\n\n    epoch_loss = running_loss / len(train_loader.dataset)\n    history['train_loss'].append(epoch_loss)\n\n    # --- 2. Validation Phase ---\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * images.size(0)\n            \n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n    epoch_val_loss = val_loss / len(val_loader.dataset)\n    epoch_acc = 100 * correct / total\n    \n    history['val_loss'].append(epoch_val_loss)\n    history['val_acc'].append(epoch_acc)\n\n    # Print Clean Stats\n    print(f\"Epoch [{epoch+1}/{num_epochs}]  \"\n          f\"Train Loss: {epoch_loss:.4f} | \"\n          f\"Val Loss: {epoch_val_loss:.4f} | \"\n          f\"Val Acc: {epoch_acc:.2f}%\")\n\n    # ==========================================\n    # EARLY STOPPING LOGIC\n    # ==========================================\n    # Check if this validation loss is the best we've seen\n    if epoch_val_loss < (best_val_loss - min_delta):\n        best_val_loss = epoch_val_loss\n        early_stop_counter = 0 # Reset counter\n        torch.save(model.state_dict(), 'best_skin_cancer_model.pth')\n        print(f\"   ‚úÖ Validation Loss Improved. Model Saved.\")\n    else:\n        early_stop_counter += 1\n        print(f\"   ‚ö†Ô∏è No improvement for {early_stop_counter}/{patience} epochs.\")\n        \n    if early_stop_counter >= patience:\n        print(f\"\\nüõë Early Stopping Triggered! Training stopped at Epoch {epoch+1}.\")\n        break\n\nprint(\"Training Finished.\")\n\n# ==========================================\n# VISUALIZATION\n# ==========================================\ndef plot_training_history(history):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n    # Plot Loss\n    ax1.plot(history['train_loss'], label='Train Loss')\n    ax1.plot(history['val_loss'], label='Val Loss')\n    ax1.set_title('Loss History')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n    ax1.grid(True)\n\n    # Plot Accuracy\n    ax2.plot(history['val_acc'], label='Val Accuracy', color='green')\n    ax2.set_title('Validation Accuracy')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy (%)')\n    ax2.legend()\n    ax2.grid(True)\n\n    plt.show()\n\n# Run the plot\nplot_training_history(history)      \n       ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T09:38:05.666277Z","iopub.execute_input":"2025-12-22T09:38:05.666469Z"}},"outputs":[{"name":"stdout","text":"üöÄ Starting training on cuda with Early Stopping...\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [1/60]  Train Loss: 1.4559 | Val Loss: 1.0507 | Val Acc: 54.40%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [2/60]  Train Loss: 1.2991 | Val Loss: 0.9853 | Val Acc: 59.95%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [3/60]  Train Loss: 1.2163 | Val Loss: 0.8528 | Val Acc: 66.95%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [4/60]  Train Loss: 1.1876 | Val Loss: 1.0196 | Val Acc: 59.62%\n   ‚ö†Ô∏è No improvement for 1/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [5/60]  Train Loss: 1.1324 | Val Loss: 0.8192 | Val Acc: 70.13%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [6/60]  Train Loss: 1.1172 | Val Loss: 0.8163 | Val Acc: 70.19%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [7/60]  Train Loss: 1.0804 | Val Loss: 0.8612 | Val Acc: 65.50%\n   ‚ö†Ô∏è No improvement for 1/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [8/60]  Train Loss: 1.0611 | Val Loss: 0.8345 | Val Acc: 63.05%\n   ‚ö†Ô∏è No improvement for 2/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch [9/60]  Train Loss: 1.0232 | Val Loss: 0.7931 | Val Acc: 70.26%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [10/60]  Train Loss: 1.0102 | Val Loss: 0.7955 | Val Acc: 68.14%\n   ‚ö†Ô∏è No improvement for 1/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [11/60]  Train Loss: 0.9703 | Val Loss: 0.7564 | Val Acc: 71.45%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [12/60]  Train Loss: 0.9352 | Val Loss: 0.7205 | Val Acc: 73.96%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [13/60]  Train Loss: 0.9549 | Val Loss: 0.7403 | Val Acc: 73.30%\n   ‚ö†Ô∏è No improvement for 1/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [14/60]  Train Loss: 0.9171 | Val Loss: 0.7572 | Val Acc: 71.32%\n   ‚ö†Ô∏è No improvement for 2/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [15/60]  Train Loss: 0.9226 | Val Loss: 0.7145 | Val Acc: 74.62%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [16/60]  Train Loss: 0.9017 | Val Loss: 0.8817 | Val Acc: 68.74%\n   ‚ö†Ô∏è No improvement for 1/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [17/60]  Train Loss: 0.9020 | Val Loss: 0.7549 | Val Acc: 69.33%\n   ‚ö†Ô∏è No improvement for 2/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [18/60]  Train Loss: 0.8522 | Val Loss: 0.7269 | Val Acc: 73.43%\n   ‚ö†Ô∏è No improvement for 3/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [19/60]  Train Loss: 0.8516 | Val Loss: 0.7953 | Val Acc: 70.65%\n   ‚ö†Ô∏è No improvement for 4/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [20/60]  Train Loss: 0.8396 | Val Loss: 0.7029 | Val Acc: 73.76%\n   ‚úÖ Validation Loss Improved. Model Saved.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [21/60]  Train Loss: 0.8397 | Val Loss: 0.7226 | Val Acc: 72.11%\n   ‚ö†Ô∏è No improvement for 1/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch [22/60]  Train Loss: 0.8098 | Val Loss: 0.7188 | Val Acc: 74.69%\n   ‚ö†Ô∏è No improvement for 2/10 epochs.\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":" # Confusion Matrix\nmodel = skinCancerCNN.to(device)\ny_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n\n# Load the weights from the file saved during training\n# (This requires 'best_skin_cancer_model.pth' to exist in your current folder)\ntry:\n    model.load_state_dict(torch.load('best_skin_cancer_model.pth'))\n    print(\"‚úÖ Successfully loaded the best saved model.\")\nexcept FileNotFoundError:\n    print(\"‚ö†Ô∏è File not found. Using current model weights instead.\")\n\nmodel.eval() # Set to evaluation mode (Turns off Dropout)\n\n# 2. PREDICTION LOOP: Get all predictions for the validation set\ny_true = []\ny_pred = []\n\nprint(\"üîÑ Processing validation set for Confusion Matrix...\")\n\nwith torch.no_grad(): # Save memory, we don't need gradients here\n    for images, labels in val_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        \n        # Get the class with the highest score\n        _, predicted = torch.max(outputs, 1)\n        \n        # Move back to CPU and convert to numpy for Sklearn\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(predicted.cpu().numpy())\n\nprint(\"‚úÖ Predictions complete.\")\n\n# 3. COMPUTE & PLOT: Create the Matrix\ncm = confusion_matrix(y_true, y_pred)\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=y_labels, yticklabels=y_labels)\nplt.xlabel('Predicted Label', fontsize=12)\nplt.ylabel('True Label', fontsize=12)\nplt.title('Confusion Matrix', fontsize=15)\nplt.show()\n\n# 4. REPORT: Print Precision, Recall, and F1-Score\nprint(\"\\nüìã Classification Report:\\n\")\nprint(classification_report(y_true, y_pred, target_names=y_labels))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}